<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Sandwich (WebRTC)</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Geist+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        * {
            box-sizing: border-box;
        }
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            margin: 0;
            padding: 24px;
            background: #fafafa;
            min-height: 100vh;
            color: #171717;
        }
        .container {
            max-width: 1100px;
            margin: 0 auto;
            display: grid;
            grid-template-columns: 400px 1fr;
            gap: 24px;
        }
        @media (max-width: 900px) {
            .container {
                grid-template-columns: 1fr;
            }
        }
        .panel {
            background: #fff;
            border: 1px solid #e5e5e5;
            border-radius: 12px;
            padding: 24px;
        }
        .controls-panel {
            display: flex;
            flex-direction: column;
        }
        h1 { 
            margin: 0 0 24px 0;
            font-size: 20px;
            font-weight: 600;
            color: #171717;
            display: flex;
            align-items: center;
            gap: 8px;
        }
        .transport-badge {
            font-size: 11px;
            font-weight: 500;
            background: #dbeafe;
            color: #1d4ed8;
            padding: 3px 8px;
            border-radius: 4px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        .button-group {
            display: flex;
            gap: 12px;
            margin-bottom: 20px;
        }
        button {
            padding: 10px 16px;
            font-size: 14px;
            font-weight: 500;
            font-family: inherit;
            cursor: pointer;
            border-radius: 8px;
            transition: all 0.15s ease;
            flex: 1;
        }
        #startBtn { 
            background: #171717;
            color: #fff;
            border: 1px solid #171717;
        }
        #startBtn:hover:not(:disabled) {
            background: #404040;
            border-color: #404040;
        }
        #startBtn:disabled { 
            background: #f5f5f5;
            color: #a3a3a3;
            border-color: #e5e5e5;
            cursor: not-allowed;
        }
        #stopBtn { 
            background: #fff;
            color: #171717;
            border: 1px solid #e5e5e5;
        }
        #stopBtn:hover:not(:disabled) {
            background: #fafafa;
            border-color: #d4d4d4;
        }
        #stopBtn:disabled { 
            background: #fafafa;
            color: #a3a3a3;
            border-color: #e5e5e5;
            cursor: not-allowed;
        }
        .status-bar {
            display: flex;
            align-items: center;
            gap: 8px;
            margin-bottom: 16px;
            padding: 10px 12px;
            background: #fafafa;
            border: 1px solid #e5e5e5;
            border-radius: 8px;
            font-size: 13px;
            color: #525252;
        }
        .status-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #a3a3a3;
        }
        .status-dot.connected {
            background: #22c55e;
        }
        .status-dot.recording {
            background: #ef4444;
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        .log-header {
            font-size: 12px;
            font-weight: 500;
            color: #737373;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 8px;
        }
        .log {
            text-align: left;
            background: #fafafa;
            border: 1px solid #e5e5e5;
            padding: 12px;
            height: 280px;
            overflow-y: auto;
            border-radius: 8px;
            font-family: 'Geist Mono', 'SF Mono', monospace;
            font-size: 12px;
            color: #525252;
        }
        .log div {
            padding: 3px 0;
            border-bottom: 1px solid #f5f5f5;
        }
        .log div:last-child {
            border-bottom: none;
        }
        pipeline-visualizer {
            display: block;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="panel controls-panel">
            <h1>ðŸ¥ª Voice Sandwich <span class="transport-badge">WebRTC</span></h1>
            <div class="button-group">
                <button id="startBtn">Start Recording</button>
                <button id="stopBtn" disabled>Stop</button>
            </div>
            <div class="status-bar">
                <span class="status-dot" id="statusDot"></span>
                <span id="status">Ready</span>
            </div>
            <div class="log-header">Console</div>
            <div class="log" id="log"></div>
        </div>
        <pipeline-visualizer></pipeline-visualizer>
    </div>
    
    <script src="/pipeline-visualizer.js"></script>

    <script>
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const statusDiv = document.getElementById('status');
        const statusDot = document.getElementById('statusDot');
        const logDiv = document.getElementById('log');
        
        // WebRTC and signaling
        let signalingWs;
        let peerConnection;
        let audioDataChannel;
        
        // Audio handling
        let audioContext;
        let workletNode;
        let mediaStream;
        let nextStartTime = 0;
        
        // Track scheduled audio sources for barge-in interruption
        let scheduledAudioSources = [];

        const TARGET_SAMPLE_RATE = 16000;

        // ICE servers configuration
        const iceServers = [
            { urls: 'stun:stun.l.google.com:19302' },
            { urls: 'stun:stun1.l.google.com:19302' },
        ];

        function log(msg) {
            const p = document.createElement('div');
            p.textContent = `${new Date().toLocaleTimeString()} â†’ ${msg}`;
            logDiv.appendChild(p);
            logDiv.scrollTop = logDiv.scrollHeight;
        }

        function setStatus(text, state = 'idle') {
            statusDiv.innerText = text;
            statusDot.className = 'status-dot';
            if (state === 'connected') statusDot.classList.add('connected');
            if (state === 'recording') statusDot.classList.add('recording');
        }

        // AudioWorkletProcessor code as a string (will be loaded via Blob URL)
        const workletCode = `
            class PCMProcessor extends AudioWorkletProcessor {
                constructor() {
                    super();
                    this.buffer = [];
                    this.targetSampleRate = 16000;
                    // sampleRate is the AudioContext's sample rate (usually 48000)
                    this.resampleRatio = sampleRate / this.targetSampleRate;
                    this.resampleBuffer = [];
                    this.resampleIndex = 0;
                }

                process(inputs) {
                    const input = inputs[0];
                    if (!input || !input[0]) return true;

                    const channelData = input[0]; // Mono or first channel

                    // Downsample from native rate to 16kHz
                    for (let i = 0; i < channelData.length; i++) {
                        this.resampleIndex += 1;
                        if (this.resampleIndex >= this.resampleRatio) {
                            this.resampleIndex -= this.resampleRatio;
                            
                            // Convert Float32 [-1, 1] to Int16 [-32768, 32767]
                            let sample = channelData[i];
                            sample = Math.max(-1, Math.min(1, sample));
                            const int16 = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                            this.buffer.push(int16);
                        }
                    }

                    // Send chunks of ~100ms worth of audio (1600 samples at 16kHz)
                    const CHUNK_SIZE = 1600;
                    while (this.buffer.length >= CHUNK_SIZE) {
                        const chunk = this.buffer.splice(0, CHUNK_SIZE);
                        const int16Array = new Int16Array(chunk);
                        this.port.postMessage(int16Array.buffer, [int16Array.buffer]);
                    }

                    return true;
                }
            }

            registerProcessor('pcm-processor', PCMProcessor);
        `;

        async function setupAudioWorklet() {
            // Create AudioContext
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            
            // Create Blob URL for the worklet
            const blob = new Blob([workletCode], { type: 'application/javascript' });
            const workletUrl = URL.createObjectURL(blob);
            
            // Load the worklet module
            await audioContext.audioWorklet.addModule(workletUrl);
            URL.revokeObjectURL(workletUrl);
            
            log(`Audio initialized (${audioContext.sampleRate}Hz)`);
        }

        async function setupMicrophone() {
            try {
                // Setup AudioWorklet if not already done
                if (!audioContext) {
                    await setupAudioWorklet();
                }
                
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Get microphone stream
                mediaStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                    }
                });
                log('Microphone active');
                
                // Create source from microphone
                const source = audioContext.createMediaStreamSource(mediaStream);
                
                // Create worklet node
                workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
                
                // Handle PCM data from worklet - send via WebRTC data channel
                workletNode.port.onmessage = (event) => {
                    if (audioDataChannel && audioDataChannel.readyState === 'open') {
                        audioDataChannel.send(event.data);
                    }
                };
                
                // Connect: microphone -> worklet
                source.connect(workletNode);
                // Don't connect to destination (we don't want to hear ourselves)
                
                log('Streaming audio via WebRTC (16kHz PCM)');
            } catch (err) {
                console.error(err);
                log('Microphone error: ' + err.message);
                setStatus('Error', 'idle');
                resetUI();
            }
        }

        // Clear all scheduled audio (for barge-in)
        function clearAudioBuffer() {
            log('Clearing audio buffer (barge-in)');
            
            // Stop all scheduled audio sources
            for (const source of scheduledAudioSources) {
                try {
                    source.stop();
                } catch (e) {
                    // Source might have already finished, ignore
                }
            }
            scheduledAudioSources = [];
            
            // Reset next start time to allow immediate playback of new audio
            nextStartTime = 0;
        }

        function handleIncomingAudio(data) {
            // We expect audio buffers from the server (PCM 16000Hz 16-bit Mono)
            if (data instanceof ArrayBuffer) {
                (async () => {
                    if (!audioContext) {
                        audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    }
                    if (audioContext.state === 'suspended') {
                        await audioContext.resume();
                    }

                    try {
                        // Ensure even byte length for Int16
                        let buffer = data;
                        if (buffer.byteLength % 2 !== 0) {
                            console.warn("Received odd byte length audio chunk, trimming last byte");
                            buffer = buffer.slice(0, buffer.byteLength - 1);
                        }
                        
                        const int16Data = new Int16Array(buffer);
                        const float32Data = new Float32Array(int16Data.length);
                        
                        // Convert Int16 to Float32
                        for (let i = 0; i < int16Data.length; i++) {
                            float32Data[i] = int16Data[i] / 32768.0;
                        }

                        // Create buffer with 1 channel, length, and 16000 sample rate
                        const audioBuffer = audioContext.createBuffer(1, float32Data.length, 16000);
                        audioBuffer.getChannelData(0).set(float32Data);

                        const source = audioContext.createBufferSource();
                        source.buffer = audioBuffer;
                        source.connect(audioContext.destination);

                        const now = audioContext.currentTime;
                        // Add a small jitter buffer (50ms) if we are starting fresh or fell behind
                        const bufferingDelay = 0.05; 
                        
                        let startTime = nextStartTime;
                        if (startTime < now) {
                            startTime = now + bufferingDelay;
                        }
                        
                        source.start(startTime);
                        nextStartTime = startTime + audioBuffer.duration;
                        
                        // Track this source for potential barge-in cancellation
                        scheduledAudioSources.push(source);
                        
                        // Clean up finished sources from the list
                        source.onended = () => {
                            const idx = scheduledAudioSources.indexOf(source);
                            if (idx > -1) {
                                scheduledAudioSources.splice(idx, 1);
                            }
                        };
                    } catch (e) {
                        console.error('Error processing audio chunk', e);
                    }
                })();
            }
        }
        
        function handleDataChannelMessage(data) {
            if (data instanceof ArrayBuffer) {
                // Binary data = audio
                handleIncomingAudio(data);
            } else if (typeof data === 'string') {
                // JSON control message
                try {
                    const message = JSON.parse(data);
                    if (message.type === 'clear-audio') {
                        clearAudioBuffer();
                    }
                } catch (e) {
                    console.warn('Unknown string message:', data);
                }
            }
        }

        startBtn.onclick = async () => {
            setStatus('Connecting...', 'idle');
            startBtn.disabled = true;
            
            // Connect to signaling WebSocket
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            signalingWs = new WebSocket(`${protocol}//${window.location.host}/ws/signaling`);
            
            signalingWs.onopen = async () => {
                log('Signaling connected');
                
                try {
                    // Create RTCPeerConnection
                    peerConnection = new RTCPeerConnection({ iceServers });
                    
                    // Handle ICE candidates
                    peerConnection.onicecandidate = (event) => {
                        if (event.candidate && signalingWs.readyState === WebSocket.OPEN) {
                            signalingWs.send(JSON.stringify({
                                type: 'ice-candidate',
                                candidate: event.candidate,
                            }));
                        }
                    };
                    
                    peerConnection.oniceconnectionstatechange = () => {
                        log(`ICE: ${peerConnection.iceConnectionState}`);
                    };
                    
                    peerConnection.onconnectionstatechange = () => {
                        log(`Connection: ${peerConnection.connectionState}`);
                        if (peerConnection.connectionState === 'connected') {
                            setStatus('Recording', 'recording');
                        } else if (peerConnection.connectionState === 'disconnected' || 
                                   peerConnection.connectionState === 'failed') {
                            setStatus('Disconnected', 'idle');
                            resetUI();
                        }
                    };
                    
                    // Create data channel for audio (client creates, server receives)
                    audioDataChannel = peerConnection.createDataChannel('audio', {
                        ordered: true,
                        maxRetransmits: 3,
                    });
                    
                    audioDataChannel.binaryType = 'arraybuffer';
                    
                    audioDataChannel.onopen = async () => {
                        log('Audio channel open');
                        stopBtn.disabled = false;
                        // Setup microphone after data channel is ready
                        await setupMicrophone();
                    };
                    
                    audioDataChannel.onmessage = (event) => {
                        handleDataChannelMessage(event.data);
                    };
                    
                    audioDataChannel.onclose = () => {
                        log('Audio channel closed');
                    };
                    
                    audioDataChannel.onerror = (err) => {
                        console.error('Data channel error:', err);
                        log('Audio channel error');
                    };
                    
                    // Create and send offer
                    const offer = await peerConnection.createOffer();
                    await peerConnection.setLocalDescription(offer);
                    
                    signalingWs.send(JSON.stringify({
                        type: 'offer',
                        sdp: peerConnection.localDescription,
                    }));
                    log('Sent WebRTC offer');
                    
                } catch (err) {
                    console.error('WebRTC setup error:', err);
                    log('WebRTC error: ' + err.message);
                    setStatus('Error', 'idle');
                    resetUI();
                }
            };
            
            signalingWs.onmessage = async (event) => {
                try {
                    const message = JSON.parse(event.data);
                    
                    if (message.type === 'answer') {
                        // Received SDP answer from server
                        log('Received WebRTC answer');
                        await peerConnection.setRemoteDescription(
                            new RTCSessionDescription(message.sdp)
                        );
                    } else if (message.type === 'ice-candidate') {
                        // Received ICE candidate from server
                        if (message.candidate) {
                            await peerConnection.addIceCandidate(message.candidate);
                        }
                    }
                } catch (e) {
                    console.error('Error processing signaling message:', e);
                }
            };

            signalingWs.onclose = () => {
                log('Signaling disconnected');
                setStatus('Disconnected', 'idle');
                resetUI();
            };
            
            signalingWs.onerror = (e) => {
                console.error(e);
                log('Signaling error');
                setStatus('Error', 'idle');
            };
        };

        stopBtn.onclick = () => {
            log('Stopping...');
            
            // Disconnect worklet
            if (workletNode) {
                workletNode.disconnect();
                workletNode = null;
            }
            
            // Stop media stream tracks
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            
            // Close data channel
            if (audioDataChannel) {
                audioDataChannel.close();
                audioDataChannel = null;
            }
            
            // Close peer connection
            if (peerConnection) {
                peerConnection.close();
                peerConnection = null;
            }
            
            // Close signaling WebSocket
            if (signalingWs) {
                signalingWs.close();
                signalingWs = null;
            }

            resetUI();
        };

        function resetUI() {
            startBtn.disabled = false;
            stopBtn.disabled = true;
            setStatus('Ready', 'idle');
            // We do NOT reset nextStartTime here because audio might still be queued playing!
            // We do NOT close audioContext because that kills pending audio and worklet.
        }
    </script>
</body>
</html>
