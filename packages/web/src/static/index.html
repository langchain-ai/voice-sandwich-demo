<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Sandwich</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Geist+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        * {
            box-sizing: border-box;
        }
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            margin: 0;
            padding: 24px;
            background: #fafafa;
            min-height: 100vh;
            color: #171717;
        }
        .container {
            max-width: 1100px;
            margin: 0 auto;
            display: grid;
            grid-template-columns: 400px 1fr;
            gap: 24px;
        }
        @media (max-width: 900px) {
            .container {
                grid-template-columns: 1fr;
            }
        }
        .panel {
            background: #fff;
            border: 1px solid #e5e5e5;
            border-radius: 12px;
            padding: 24px;
        }
        .controls-panel {
            display: flex;
            flex-direction: column;
        }
        h1 { 
            margin: 0 0 24px 0;
            font-size: 20px;
            font-weight: 600;
            color: #171717;
            display: flex;
            align-items: center;
            gap: 8px;
        }
        .button-group {
            display: flex;
            gap: 12px;
            margin-bottom: 20px;
        }
        button {
            padding: 10px 16px;
            font-size: 14px;
            font-weight: 500;
            font-family: inherit;
            cursor: pointer;
            border-radius: 8px;
            transition: all 0.15s ease;
            flex: 1;
        }
        #startBtn { 
            background: #171717;
            color: #fff;
            border: 1px solid #171717;
        }
        #startBtn:hover:not(:disabled) {
            background: #404040;
            border-color: #404040;
        }
        #startBtn:disabled { 
            background: #f5f5f5;
            color: #a3a3a3;
            border-color: #e5e5e5;
            cursor: not-allowed;
        }
        #stopBtn { 
            background: #fff;
            color: #171717;
            border: 1px solid #e5e5e5;
        }
        #stopBtn:hover:not(:disabled) {
            background: #fafafa;
            border-color: #d4d4d4;
        }
        #stopBtn:disabled { 
            background: #fafafa;
            color: #a3a3a3;
            border-color: #e5e5e5;
            cursor: not-allowed;
        }
        .status-bar {
            display: flex;
            align-items: center;
            gap: 8px;
            margin-bottom: 16px;
            padding: 10px 12px;
            background: #fafafa;
            border: 1px solid #e5e5e5;
            border-radius: 8px;
            font-size: 13px;
            color: #525252;
        }
        .status-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #a3a3a3;
        }
        .status-dot.connected {
            background: #22c55e;
        }
        .status-dot.recording {
            background: #ef4444;
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        .log-header {
            font-size: 12px;
            font-weight: 500;
            color: #737373;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 8px;
        }
        .log {
            text-align: left;
            background: #fafafa;
            border: 1px solid #e5e5e5;
            padding: 12px;
            height: 280px;
            overflow-y: auto;
            border-radius: 8px;
            font-family: 'Geist Mono', 'SF Mono', monospace;
            font-size: 12px;
            color: #525252;
        }
        .log div {
            padding: 3px 0;
            border-bottom: 1px solid #f5f5f5;
        }
        .log div:last-child {
            border-bottom: none;
        }
        pipeline-visualizer {
            display: block;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="panel controls-panel">
            <h1>ðŸ¥ª Voice Sandwich</h1>
            <div class="button-group">
                <button id="startBtn">Start Recording</button>
                <button id="stopBtn" disabled>Stop</button>
            </div>
            <div class="status-bar">
                <span class="status-dot" id="statusDot"></span>
                <span id="status">Ready</span>
            </div>
            <div class="log-header">Console</div>
            <div class="log" id="log"></div>
        </div>
        <pipeline-visualizer></pipeline-visualizer>
    </div>
    
    <script src="/pipeline-visualizer.js"></script>

    <script>
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const statusDiv = document.getElementById('status');
        const statusDot = document.getElementById('statusDot');
        const logDiv = document.getElementById('log');
        
        let ws;
        let audioContext;
        let workletNode;
        let mediaStream;
        let nextStartTime = 0;

        const TARGET_SAMPLE_RATE = 16000;

        function log(msg) {
            const p = document.createElement('div');
            p.textContent = `${new Date().toLocaleTimeString()} â†’ ${msg}`;
            logDiv.appendChild(p);
            logDiv.scrollTop = logDiv.scrollHeight;
        }

        function setStatus(text, state = 'idle') {
            statusDiv.innerText = text;
            statusDot.className = 'status-dot';
            if (state === 'connected') statusDot.classList.add('connected');
            if (state === 'recording') statusDot.classList.add('recording');
        }

        // AudioWorkletProcessor code as a string (will be loaded via Blob URL)
        const workletCode = `
            class PCMProcessor extends AudioWorkletProcessor {
                constructor() {
                    super();
                    this.buffer = [];
                    this.targetSampleRate = 16000;
                    // sampleRate is the AudioContext's sample rate (usually 48000)
                    this.resampleRatio = sampleRate / this.targetSampleRate;
                    this.resampleBuffer = [];
                    this.resampleIndex = 0;
                }

                process(inputs) {
                    const input = inputs[0];
                    if (!input || !input[0]) return true;

                    const channelData = input[0]; // Mono or first channel

                    // Downsample from native rate to 16kHz
                    for (let i = 0; i < channelData.length; i++) {
                        this.resampleIndex += 1;
                        if (this.resampleIndex >= this.resampleRatio) {
                            this.resampleIndex -= this.resampleRatio;
                            
                            // Convert Float32 [-1, 1] to Int16 [-32768, 32767]
                            let sample = channelData[i];
                            sample = Math.max(-1, Math.min(1, sample));
                            const int16 = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
                            this.buffer.push(int16);
                        }
                    }

                    // Send chunks of ~100ms worth of audio (1600 samples at 16kHz)
                    const CHUNK_SIZE = 1600;
                    while (this.buffer.length >= CHUNK_SIZE) {
                        const chunk = this.buffer.splice(0, CHUNK_SIZE);
                        const int16Array = new Int16Array(chunk);
                        this.port.postMessage(int16Array.buffer, [int16Array.buffer]);
                    }

                    return true;
                }
            }

            registerProcessor('pcm-processor', PCMProcessor);
        `;

        async function setupAudioWorklet() {
            // Create AudioContext
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            
            // Create Blob URL for the worklet
            const blob = new Blob([workletCode], { type: 'application/javascript' });
            const workletUrl = URL.createObjectURL(blob);
            
            // Load the worklet module
            await audioContext.audioWorklet.addModule(workletUrl);
            URL.revokeObjectURL(workletUrl);
            
            log(`Audio initialized (${audioContext.sampleRate}Hz)`);
        }

        startBtn.onclick = async () => {
            setStatus('Connecting...', 'idle');
            startBtn.disabled = true;
            
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            ws = new WebSocket(`${protocol}//${window.location.host}/ws`);
            ws.binaryType = 'arraybuffer';
            
            ws.onopen = async () => {
                setStatus('Recording', 'recording');
                log('Connected to server');
                stopBtn.disabled = false;

                try {
                    // Setup AudioWorklet if not already done
                    if (!audioContext) {
                        await setupAudioWorklet();
                    }
                    
                    if (audioContext.state === 'suspended') {
                        await audioContext.resume();
                    }

                    // Get microphone stream
                    mediaStream = await navigator.mediaDevices.getUserMedia({ 
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true,
                        }
                    });
                    log('Microphone active');
                    
                    // Create source from microphone
                    const source = audioContext.createMediaStreamSource(mediaStream);
                    
                    // Create worklet node
                    workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
                    
                    // Handle PCM data from worklet
                    workletNode.port.onmessage = (event) => {
                        if (ws && ws.readyState === WebSocket.OPEN) {
                            ws.send(event.data);
                        }
                    };
                    
                    // Connect: microphone -> worklet
                    source.connect(workletNode);
                    // Don't connect to destination (we don't want to hear ourselves)
                    
                    log('Streaming audio (16kHz PCM)');
                } catch (err) {
                    console.error(err);
                    log('Microphone error: ' + err.message);
                    setStatus('Error', 'idle');
                    resetUI();
                }
            };

            ws.onmessage = async (event) => {
                // We expect audio buffers from the server (PCM 16000Hz 16-bit Mono)
                if (event.data instanceof ArrayBuffer) {
                    if (!audioContext) {
                        audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    }
                    if (audioContext.state === 'suspended') {
                        await audioContext.resume();
                    }

                    try {
                        // Ensure even byte length for Int16
                        let buffer = event.data;
                        if (buffer.byteLength % 2 !== 0) {
                            console.warn("Received odd byte length audio chunk, trimming last byte");
                            buffer = buffer.slice(0, buffer.byteLength - 1);
                        }
                        
                        const int16Data = new Int16Array(buffer);
                        const float32Data = new Float32Array(int16Data.length);
                        
                        // Convert Int16 to Float32
                        for (let i = 0; i < int16Data.length; i++) {
                            float32Data[i] = int16Data[i] / 32768.0;
                        }

                        // Create buffer with 1 channel, length, and 16000 sample rate
                        const audioBuffer = audioContext.createBuffer(1, float32Data.length, 16000);
                        audioBuffer.getChannelData(0).set(float32Data);

                        const source = audioContext.createBufferSource();
                        source.buffer = audioBuffer;
                        source.connect(audioContext.destination);

                        const now = audioContext.currentTime;
                        // Add a small jitter buffer (50ms) if we are starting fresh or fell behind
                        const bufferingDelay = 0.05; 
                        
                        let startTime = nextStartTime;
                        if (startTime < now) {
                            startTime = now + bufferingDelay;
                        }
                        
                        source.start(startTime);
                        nextStartTime = startTime + audioBuffer.duration;
                    } catch (e) {
                        console.error('Error processing audio chunk', e);
                    }
                } else {
                    log('Server: ' + event.data);
                }
            };

            ws.onclose = () => {
                setStatus('Disconnected', 'idle');
                log('Connection closed');
                resetUI();
            };
            
            ws.onerror = (e) => {
                console.error(e);
                log('Connection error');
                setStatus('Error', 'idle');
            };
        };

        stopBtn.onclick = () => {
            log('Stopping...');
            
            // Disconnect worklet
            if (workletNode) {
                workletNode.disconnect();
                workletNode = null;
            }
            
            // Stop media stream tracks
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }

            resetUI();
        };

        function resetUI() {
            startBtn.disabled = false;
            stopBtn.disabled = true;
            setStatus('Ready', 'idle');
            // We do NOT reset nextStartTime here because audio might still be queued playing!
            // We do NOT close audioContext because that kills pending audio and worklet.
        }
    </script>
</body>
</html>
